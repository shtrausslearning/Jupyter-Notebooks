{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport numpy as np\nimport random as rn\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom PIL import Image\n%matplotlib inline\nimport time\nfrom keras import models\nfrom tensorflow.keras import applications as app\nfrom tensorflow.keras.models import Sequential # Sequential model initialization\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten,AveragePooling2D\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\nfrom tqdm.keras import TqdmCallback\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # For data pipeline building\nfrom functools import partial\nimport keras\n\nlabels = 4       \nsshape = (224,224,3)\nn_epochs = 60\nseed = 221\nos.environ['PYTHONHASHSEED'] = '0'                      \nnp.random.seed(seed)\nrn.seed(seed)\ntf.random.set_seed(seed)\n\n# Palettes\nlst_color = ['#B1D784','#2E8486','#004379','#032B52','#EAEA8A']\n\n# Folder Pathways\nmain_folder = '/kaggle/input/hummingbirds-at-my-feeders/'\ntrain_folder = '/kaggle/input/hummingbirds-at-my-feeders/hummingbirds/train/'\nval_folder = '/kaggle/input/hummingbirds-at-my-feeders/hummingbirds/valid/'\ntest_folder = '/kaggle/input/hummingbirds-at-my-feeders/hummingbirds/test/'\nvideo_folder = '/kaggle/input/hummingbirds-at-my-feeders/video_test/'\n\n''' Helper Functions '''\n# Plot keras history output once it has been define globally\n\ndef HistPlot(title):\n\n    import seaborn as sns; sns.set(style='whitegrid')\n    import matplotlib.pyplot as plt\n\n    fig,ax = plt.subplots(1,2,figsize=(12,4))\n    sns.despine(top=True,left=True,bottom=True)\n\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('model accuracy')\n    ax[0].set_ylabel('accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].grid(True,linestyle='--',alpha=0.5)\n    \n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('model loss')\n    ax[1].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].legend(['train', 'test'], loc='upper left')\n    ax[1].grid(True,linestyle='--',alpha=0.5)\n\n    plt.title(title)\n    plt.show()\n    \n# Plot \n\ndef show_grid(image_list,nrows,ncols,label_list=None,show_labels=False,savename=None,figsize=(20,10),showaxis='off'):\n    if type(image_list) is not list:\n        if(image_list.shape[-1]==1):\n            image_list = [image_list[i,:,:,0] for i in range(image_list.shape[0])]\n        elif(image_list.shape[-1]==3):\n            image_list = [image_list[i,:,:,:] for i in range(image_list.shape[0])]\n    fig = plt.figure(None, figsize,frameon=False)\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(nrows, ncols),  # creates 2x2 grid of axes\n                     axes_pad=0.3,  # pad between axes in inch.\n                     share_all=True,\n                     )\n    for i in range(nrows*ncols):\n        ax = grid[i]\n        img = Image.open(image_list[i])\n        ax.imshow(img,cmap='Greys_r')  # The AxesGrid object work as a list of axes.\n        ax.axis('off')\n        if show_labels:\n            ax.set_title(class_mapping[y_int[i]])\n    if savename != None:\n        plt.savefig(savename,bbox_inches='tight')\n    \n# https://keras.io/api/preprocessing/image/","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **HUMMINGBIRD CLASSIFICATION**\nFor [Hummingbirds at my feeders](https://www.kaggle.com/akimball002/hummingbirds-at-my-feeders) dataset, Wonderful Photo by [Zdeněk Macháček](https://unsplash.com/@zmachacek)\n<b>As a sidenote:</b> If you use art/photography in your notebooks, please have the courtesy to give credit to the artist in question & make sure the artwork release license allows you to use it. Artwork is not a code, every piece is created with a specific individual style & a lot of work goes into the creation of every piece.\n\n# 1. <span style='color:#B6DA32 '> INTRODUCTION </span>\n\n![banner](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8cc1eeaa-4046-4c4a-ae93-93d656f68688/dehie0n-a2e641c6-af9a-47cf-832c-8015e86b7347.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOiIsImlzcyI6InVybjphcHA6Iiwib2JqIjpbW3sicGF0aCI6IlwvZlwvOGNjMWVlYWEtNDA0Ni00YzRhLWFlOTMtOTNkNjU2ZjY4Njg4XC9kZWhpZTBuLWEyZTY0MWM2LWFmOWEtNDdjZi04MzJjLTgwMTVlODZiNzM0Ny5qcGcifV1dLCJhdWQiOlsidXJuOnNlcnZpY2U6ZmlsZS5kb3dubG9hZCJdfQ.CnD5mRUqdz93URXVhEPqPNQLlY5U4bq-uDVWCZ2Bn4g)\n\n<b>Bird Monitoring </b>\n\n- Having conducted a quick background investigation, it has become evident that a sizable portion of the monitoring & observation surrounding birds is mostly done manually by researchers & bird monitoring enthusiasts (Eg. you can submit your own sightings to [Hummingbirdcentral](https://www.hummingbirdcentral.com/) & you wouldn't need to look far for bird sighting in general, take [Xeno-Canto](https://www.xeno-canto.org/) for example.\n- The key the points outlined in <b>Importance of this work</b> is quantity, & with the evolution of monitoring technology, it has become quite accessable for anyone to acquire a camera/videocamera & start contributing to the overall process of bird monitoring.\n- When it comes to hummingbirds, there is a lot of interest across various parts of the United States. Enthusiasts and bird watchers often <b>set up bird feeders</b> to attract birds & one of the feeding spots ( which is used in this dataset ) is located in the state of Colorado.\n- To monitor the feeder we simply need a digital camera/recorder pointed at the feeder, the recording data can then be extracted and analysed autonomously to identify the species that were present during recording. Via separate automation or visual inspection, frames can be extrated when birds are present and a <b>classifier can be used to identify them autonomously</b>.\n\n<b>Where Machine Learning Comes In</b>\n- A key component in the bird identification process is the actual model that will need to be trained, for it to be able to identify and classify a particular bird present at the feeder.\n- Machine Learning & in particular the use of Convolution Neural Network can be used to build a classification model, so we can identify the species present at the feeder.\n- The photos collected from [akimball002](https://www.kaggle.com/akimball002) feeders' cameras with a binary bird-finder classification model to create a CNN multi-species and gender classification model. \n\n<b>Purpose, Goal & Application</b>\n\n- The <b>purpose of this project</b> is to create an <b>image classification for hummingbird species</b> and <b>genders</b> that visit feeders in Colorado. \n- The <b>ultimate goal</b> is to have a <b>classification system that can be deployed at any feeder</b>. \n- It is <b>applicable to anywhere that hummingbirds migrate or breed</b> given additional datasets for those species. \n\n<b>Importance of this work</b>\n\n- This is <b>important to the continued monitoring of hummingbird species</b> and <b>migration patterns</b>. \n- Humming bird <b>migration is otherwise reliant on individual bird watchers</b> to see and report their observations. \n- If avid bird lovers setup a similar system, then <b>conservation organizations would have better information on migratory and breeding patterns</b>. \n- This knowledge can be used to determine if specific changes in the environment or ecology has positively or negatively impacted bird life.\n\n<b>Interesting References</b>\n- [Hummingbirds 101](https://www.perkypet.com/advice/hummingbirds-101)\n- [Hummingbird Nectar Recipes](https://nationalzoo.si.edu/migratory-birds/hummingbird-nectar-recipe)\n- [Keras Image Augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation)","metadata":{}},{"cell_type":"markdown","source":"# 2. <span style='color:#B6DA32 '>DATASET </span>\n- In the context of bird monitoring, what this dataset outlines more than anything else is that you don't need to place cameras right next to the feeder, which for some species can be offputting & the images don't need to be of perfect quality in order to create a classifier that can identify hummingbird species accurately.\n- We will go through the images images in <b>Training Image Exploration</b>\n- The primary birds at the feeders this year are [broad-tailed (Selasphorous platycerus)*](https://www.allaboutbirds.org/guide/Broad-tailed_Hummingbird/id ) and a few [rufous (Selasphorous rufus)*](https://www.allaboutbirds.org/guide/Rufous_Hummingbird/) hummingbirds</b>.\n\n<b>Folder Layout</b>\n- Let's see what kind of dataset we are dealing with; there seems to be a specific folder already assembled specifically for modelling; <b>hummingbirds</b>, which contains subfolders for each separate class. When it comes time to create a dataset, this will be very convenient since we can just call the <b>.flow_from_directory</b> function.\n- There also is a folder containing \n- As well as two other folder associated with a video recording (collection of still images); <b>video_test</b> & All images in the dataset; <b>All_images</b>","metadata":{}},{"cell_type":"code","source":"os.listdir(main_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(train_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style='color:#73C382'> <sub>TRAINING IMAGE EXPLORATION </sub> </span>\nLet's view what kind of images we are dealing with.","metadata":{}},{"cell_type":"code","source":"lst_temp = os.listdir(train_folder+'/Rufous_female')\nlst_rufous_female = []\nfor i in lst_temp:\n    lst_rufous_female.append(train_folder+'/Rufous_female/'+i)\n    \nlst_temp = os.listdir(train_folder+'/Broadtailed_female')\nlst_broad_female = []\nfor i in lst_temp:\n    lst_broad_female.append(train_folder+'/Broadtailed_female/'+i)\n    \nlst_temp = os.listdir(train_folder+'/Broadtailed_male')\nlst_broad_male = []\nfor i in lst_temp:\n    lst_broad_male.append(train_folder+'/Broadtailed_male/'+i)\n    \nlst_temp = os.listdir(train_folder+'/No_bird')\nlst_none = []\nfor i in lst_temp:\n    lst_none.append(train_folder+'/No_bird/'+i)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RUFOUS FEMALE\n- The addition of the <b>female as opposed to the male</b> is an interesting choice for the dataset, making it indeed very challenging for the model, due to the high similarity of different species' image values. The female, unlike the male is very similar to the <b>broadtail female</b>, <b>especially when in the shade</b>. \n- However we can notice that in all images, colours of the back/rump are <b>quite dull, more saturated, but with hints of green</b>, just enough to be able to make out the green colour.\n- We can also note that the images have quite a <b>bit of \"noise\"</b>, that could potentally affect the accuracy of the model; <b>the feeder & the flag introduce red values into the images</b>, which can affect the model accuracy.","metadata":{}},{"cell_type":"code","source":"show_grid(lst_rufous_female,5,10,figsize=(20,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BROADTAIL FEMALE\n- To the naked eye, there is <b>a lot of similarities</b> between the <b>Rufous</b> & <b>Broadtail Females</b>\n- Without adequate lighting and refence to multiple frames (such as from a video), one could easily mislabel the species.","metadata":{}},{"cell_type":"code","source":"show_grid(lst_broad_female,5,10,figsize=(20,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BROADTAIL MALE\n- Like the adult female, the males also have green and buffy flanks.\n- What separates the male broadtail from the female and even from the Rufous female is the **distinctive rose/magenta throats**.\n- It's quite likely the model would be easily able to classify any image containing the male from the rest, if the feeder or flag wasn't of similar color.\n- We can clearly observe that the feeder, has both darker spots and lighter spots, lighter spots have values very similar values to the throat.\n- We can also note some images don't visibly have this distinctive red colour throat (at least to the naked eye), one possible reason being that the bird is <b>in the shade at the time of capture</b>. It's also possible that the images are not correctly labeled, which cannot also be ruled out. However we can clearly note that the model will need to adapt to images <b>taken under direct sunlight</b> and <b>different shade variations</b>, which create some problematic scenarios.\n- And <b>last but not least</b>, we can't rule out <b>immature males</b> from the pack as well. They are extremly similar to female as well.","metadata":{}},{"cell_type":"code","source":"show_grid(lst_broad_male,5,10,figsize=(20,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NO BIRD\n- Given the amount of background noise/clutter (non bird pixels) we have in our images, <b>no_bird images seem like significant additions</b>.\n- Especially important are the <b>flag</b> & <b>feeder</b> images which we saw in the background of some of the hummingbird images.\n- The subset is quite self explanatory, no birds are present in any of the images, showing the environment around the feeder.","metadata":{}},{"cell_type":"code","source":"show_grid(lst_none,5,10,figsize=(20,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SUMMARY\n- Most hummingbirds are very similar in shape and are <b>mostly differentiable by their colours</b>, so one channel CNN input would be less effective, and we have to rely on all colour channels to distinguish the species.\n- Having gone through the images, we can see that the current dataset is quite a challenging one. A lot of other hummingbirds, especially male have very <b>identifiable feather colours</b>, however in this dataset, aside from the <b>broadtail male</b>, most hummingbirds <b>seem amost identical to the naked eye</b>, especially in images which are slightly underexposed. As a result, it would be of great benefit to create a model that can identify the specie as accurately as possible at any given feeder.\n- The <b>model heavily relies on accurate initial training label data</b> in order to be able to accurately classify the current set of hummingbirds on unseed data, so it was assumed that the dataset labels are as accurate as possible.\n\n## <span style='color:#73C382'> <sub>MODEL TRAINING DATA </sub> </span>\n- The modelling dataset is not very big; 100 images per class for <b>training</b> & 20 images per class for <b>validation</b> and <b>test</b> evaluation, used during training and subsequently for inference.\n- The <b>classes are also well balanced</b>, containing the same number of images per class.","metadata":{}},{"cell_type":"code","source":"class_types = len(os.listdir(train_folder))\nprint('Number of classes for Classification: ',class_types)\nclass_names = os.listdir(train_folder)\nprint(f'The class names are {class_names}\\n')\n\nprint('Training dataset:')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir(train_folder+i))))\n\nprint('\\nValidation dataset:')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir(val_folder+i))))\n    \nprint('\\nTest dataset:')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir(test_folder+i))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. <span style='color:#B6DA32 '>MODEL GENERATION </span>\n\n## <span style='color:#73C382'> <sub>IMAGE AUGMENTATION & GENERATE DATASET</sub> </span>\n- One of the important factors affecting the accuracy of the CNN; <b>image augmentation</b>. We will focus our attention to the influence of augmentation quite a bit in this notebook.\n- <b>Let's start off with a baseline</b>; With a preset <b>rescale</b> image augmentation, applied to both training data, validation & test data. ie. Making sure the values lie between [0,1]; which has proven to be quite important for neural netoworks to perform well.\n- Upon defining the desired augmentation modification we will apply to the dataset, we can use the <b>flow_from_directory</b> data generator function to generate the dataset, as we already have our dataset nicely assembled into folders <b>train</b>,<b>valid</b> & <b>test</b>.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator \n\n# Define DataGenerators\ntrain_datagen = ImageDataGenerator(rescale=1.0/255)\ngen_datagen = ImageDataGenerator(rescale=1.0/255)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataGenerators via Folder Directory\ngen_train = train_datagen.flow_from_directory(train_folder, \n                        target_size=(224,224),  # target size\n                        batch_size=32,          # batch size\n                        class_mode='categorical')    # batch size\n\ngen_valid = gen_datagen.flow_from_directory(val_folder,\n                        target_size=(224,224),\n                        batch_size=32,\n                        class_mode='categorical')\n\ngen_test = gen_datagen.flow_from_directory(test_folder,\n                        target_size=(224,224),\n                        batch_size=32,\n                        class_mode='categorical')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style='color:#73C382'> <sub>TRAIN A CONVOLUTED NEURAL NETWORK</sub> </span>\n- Let's be sure to try out a less complicated neural network consisting of two convolution layers, a pooling layer and dense block of layers at first, and work out way upto more complex networks.\n- The Neural network will have to be trained from scratch, with randomly generated weights for initialisation.\n- To train a model, we need to first define a <b>model architecture</b>, then <b>compile</b> (req. <b>an optimiser</b>, <b>a loss function</b> & <b>evaluation metric</b>) it & subsequently <b>fit</b>, to train the model.\n- <b>callbacks</b> are useful as well to control various aspect of the training procedure; lets use two commonly used ones <b>ReduceLROnPlateau</b> (for learning rate adjustment) & <b>ModelCheckpoint</b> (for model saves).\n- Let's also plot the <b>evaluation metric; accuracy</b> & <b>model loss function</b>.","metadata":{}},{"cell_type":"code","source":"# Two Convolution Layer CNN\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=sshape),    \n    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n    keras.layers.MaxPool2D(),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(labels, activation=\"softmax\")\n])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Callback Options During Training '''\ncallbacks = [ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=0, \n                               factor=0.5,mode='max',min_lr=0.001),\n             ModelCheckpoint(filepath=f'model_cnn.h5',monitor='val_accuracy',\n                             mode = 'max',verbose=0,save_best_only=True),\n             TqdmCallback(verbose=1)] \n\n''' Model Compilation '''\nmodel.compile(optimizer='Adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n''' Start Training '''\nstart = time.time()\nhistory = model.fit(gen_train,\n                    validation_data = gen_valid,\n                    callbacks=callbacks,\n                    verbose=0,epochs=n_epochs)\nend = time.time()\nprint(f'The time taken to execute is {round(end-start,2)} seconds.')\nprint(f'Maximum Train/Val {max(history.history[\"accuracy\"]):.4f}/{max(history.history[\"val_accuracy\"]):.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=['Evaluation Metric: Accuracy','Model Loss'])\nfig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)],y=history.history['accuracy'],\n                         name=f'train_accuracy',line=dict(color=lst_color[0])),row=1, col=1)\nfig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)],y=history.history['val_accuracy'],\n                         name=f'val_accuracy',line=dict(color=lst_color[1])),row=1, col=1)\nfig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)],y=history.history['loss'],\n                         name=f'train_loss',line=dict(color=lst_color[0])),row=1, col=2)\nfig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)],y=history.history['val_loss'],\n                         name=f'val_loss',line=dict(color=lst_color[1])),row=1, col=2)\n    \nfig.update_layout(template='plotly_white',\n                  title='<b>Simple CNN Model Training</b>',\n                 font=dict(family='sans-serif',size=14))\nfig.update_layout(margin={\"r\":0,\"t\":100,\"l\":0,\"b\":0},height=400,showlegend=False)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SUMMARY\n- What we can see is a neural network steadily learning & reaching a <b>validation accuracy</b> peak of about 0.81. We are not applying any augmentations other than scaling so it is unlikely the model will improve past this point.\n- Next, we ought to test other augmentation additions, applied to the training of the same network.","metadata":{}},{"cell_type":"markdown","source":"## <span style='color:#73C382'> <sub>IMAGE AUGMENTATION INFLUENCE </sub> </span>\n\n- There various combinations of image augmentations that we can try in order to attempt find a good balance between overfitting and generalisation. The full collection of augmentations can be found in the [Keras manual](https://keras.io/api/preprocessing/image/). I have tried to add most of the familar ones in the generalised function.\n- In the next section, we will use a generalised function (since we want to repeat the process multiple times) and **cycle through various combinations of image augmentation** to be applied during training.\n- During each epoch, **a randomised combination of augmentation applied to each image** may lead to a spontaneous improvement in model accuracy. Aside from the perhaps adjustment of <b>brightness_range</b>, its not entirely clear what can potentially improve the model, so let's investigate.\n- The selection of augmentation is like a hyperparameter, which will affect our accuracy. We need to investigate the effects of image augmentation in order to understand how they influence the training of the model accuracy, some combinations will result in an improvement & others won't, so the evaluation metric is expected to oscillate quite a bit compared to the base model that we have already evaluated.\n- The <b>augment_model</b> is simply a function with all steps required to train a moedl, with the exception of the augmentation settings, which need to be imported via a list (<b>lst_aug</b>). ","metadata":{}},{"cell_type":"code","source":"# Evaluate CNN model w/ imported augmentation\ndef augment_model(lst_aug):\n    \n    # Requires list of augmentations to training & val/test data\n\n    # Define DataGenerators\n    gen_train = lst_aug[0].flow_from_directory(train_folder, \n                            target_size=(224,224),  # target size\n                            batch_size=32,          # batch size\n                            class_mode='categorical')    # batch size\n\n    gen_valid = lst_aug[1].flow_from_directory(val_folder,\n                            target_size=(224,224),\n                            batch_size=32,\n                            class_mode='categorical')\n\n    gen_test = lst_aug[1].flow_from_directory(test_folder,\n                            target_size=(224,224),\n                            batch_size=32,\n                            class_mode='categorical')\n\n    # Define a CNN Model\n    model = keras.models.Sequential([\n        keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=sshape),    \n        keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n        keras.layers.MaxPool2D(),\n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.25),\n        keras.layers.Dense(128, activation=\"relu\"),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(labels, activation=\"softmax\")\n    ])\n\n    # Callback Options During Training \n    callbacks = [ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=0, \n                                   factor=0.5,mode='max',min_lr=0.001),\n                 ModelCheckpoint(filepath=f'model_resnet34.h5',monitor='val_accuracy',\n                                 mode = 'max',verbose=0,save_best_only=True),\n                 TqdmCallback(verbose=1)] \n    \n    # Compile Model\n    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n    # Evaluate Model\n    history = model.fit(gen_train,\n                        validation_data = gen_valid,\n                        callbacks=callbacks,\n                        verbose=0,epochs=n_epochs)\n    \n    # Return Result History\n    return history ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Let's try four different combinations of augmentation:\n\n    - <code>rescale (1/255)</code>, <code>horizontal_flip</code>\n    - <code>rescale (1/255)</code>, <code>vertical_flip</code>\n    - <code>rescale (1/255)</code>, <code>brightness_range (+1.1,+1.5)</code>\n    - <code>rescale (1/255)</code>, <code>horizontal_flip</code>, <code>shear_range (0.2)</code>, <code>zoom_range (0.2)</code>\n    \n    \n- What we are interesting in determining: <b>which particular combination of augmentation gives us the most promising improvement in metric performance.</b>\n- Once we have this information, we can go ahead and try this augmentation on more sophisticated models, so we can potentially obtain a more accurate model.\n- We will also only plot the <b>accuracy metric</b>, both <b>training & validation</b> for each augmentation combination.","metadata":{}},{"cell_type":"code","source":"# List of Augmentation Options & default settings \nlst_augopt = ['rescale','horizontal_flip','vertical_flip',\n              'brightness_range','rotation_range','shear_range',\n              'zoom_range','width_shift_range','height_shift_range',\n              'channel_shift_range','zca_whitening','featurewise_center',\n              'samplewise_center','featurewise_std_normalization','samplewise_std_normalization']\nlst_augval = [1.0/255,True,True,  \n              [1.1,1.5],0.2,0.2,\n              0.2,0,0,\n              0,True,False,\n             False,False,False]\n\n# Select Augmentations \nlst_select = [[0,1],[0,2],[0,3],[0,1,5,6]]\nii=-1; lst_res_train = [] ; lst_res_val = []\nfor augs in lst_select:\n    \n    print('Augmentation Combination')\n    # get dictionary of augmentation options\n    ii+=1; dic_select = dict(zip([lst_augopt[i] for i in lst_select[ii]],[lst_augval[i] for i in lst_select[ii]]))\n    print(dic_select)\n\n    # define augmentation options\n    train_datagen = ImageDataGenerator(**dic_select) # pass arguments\n    gen_datagen = ImageDataGenerator(rescale=1.0/255)\n\n    # evaluate model & return history metric\n    history = augment_model([train_datagen,gen_datagen])\n\n    # store results\n    lst_res_train.append(history.history['accuracy'])  \n    lst_res_val.append(history.history['val_accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=['Training Accuracy','Validation Accuracy'])\nii=-1\nfor i in lst_select:\n    ii+=1;fig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)], \n                                   y=lst_res_train[ii],\n                                   name=f'{i}',\n                                   line=dict(color=lst_color[ii])),row=1, col=1)\nii=-1\nfor i in lst_select:\n    ii+=1;fig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)], \n                                   y=lst_res_val[ii],\n                                   name=f'{i}',\n                                   line=dict(color=lst_color[ii])),row=1, col=2)\n    \nfig.update_layout(template='plotly_white',\n                  title='<b> Image Augmentation Variations : Evaluation Metric (Accuracy)</b>',\n                 font=dict(family='sans-serif',size=14))\nfig.update_layout(margin={\"r\":0,\"t\":100,\"l\":0,\"b\":0},height=400,showlegend=False)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SUMMARY\n- Quite an interesting set of results; it was thought that <b>due to the low brightness nature of a lot of images</b>, an increase in brightness would allow the model to more easily distinguish between different classes. We can see that when the applying the increased brightness augmentation; [0,3]  set to (+1.1,+1.5), the training accuracy increased quite rapidly, potentially showing signs of overfitting, the validation chunk performed quite poorly compared to other types of augmentation.\n- What was interesting to observe was the <b>balance between training/validation accuracies</b>. Augmentations that learned slower and ended up with lower training accuracies tended to generalise better on unseen data.\n- Simple Horizontal flipping, [0,1] and the combination of four augmentations (shearing,zooming,flipping) [0,1,5,6], both were more effective than brightness augmentation adjustments [0,3].\n- Due to the large number of possible augmentation combinations, it was quite convenient to create and test a <b>list based augmentation selection approach</b>, allowing us to simple loop through the various set options. More testing would be quite interesting to try, however we should next <b>shift our attention to more sophisticated neural networks</b>, since they are more than likely going to allow us to edge the current validaiton accuracy score of our simple network.","metadata":{}},{"cell_type":"markdown","source":"## <span style='color:#73C382'> <sub>ESTABLISHED CNN ARCHITECTURES </sub> </span>\n- Having a wide range of networks available to use, let's investigate various networks and their effects on the accuracy of the overall model.\n- Aside from our own simple convolution network, let's look into established networks; <b>VGG</b>,<b>ResNet</b>,<b>MobileNet</b>,<b>Inception</b> & <b>EfficientNet</b>\n- If we use established networks, we need to adjust the network, ie. adjust the tailend of the model, so it can be used in this problem.\n- Head weights will be frozen so that the network doesn't overfit the training data.\n- Having looked at different augmentations, let's use one of the better combinations & define the dataloaders again. It's not a given that augmentations that led to a better result using the simpler model will translate to a more complex, so let's investigate.","metadata":{}},{"cell_type":"code","source":"# Define again with \ntrain_datagen = ImageDataGenerator(rescale=1.0/255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                  )\ngen_datagen = ImageDataGenerator(rescale=1.0/255)\n\ngen_train = train_datagen.flow_from_directory(train_folder, \n                        target_size=(224,224),  # target size\n                        batch_size=32,          # batch size\n                        class_mode='categorical')    # batch size\n\ngen_valid = gen_datagen.flow_from_directory(val_folder,\n                        target_size=(224,224),\n                        batch_size=32,\n                        class_mode='categorical')\n\ngen_test = gen_datagen.flow_from_directory(test_folder,\n                        target_size=(224,224),\n                        batch_size=32,\n                        class_mode='categorical')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import applications as app\ndef pretrained_model(head_id):\n\n    # Define model with different applications\n    model = Sequential()\n\n    ''' Define Head Pretrained Models '''\n\n    if(head_id is 'vgg'):\n        model.add(app.VGG16(input_shape=sshape,\n                            pooling='avg',\n                            classes=1000,\n                            include_top=False,\n                            weights='imagenet'))\n\n    elif(head_id is 'resnet'):\n        model.add(app.ResNet101(include_top=False,\n                               input_tensor=None,\n                               input_shape=sshape,\n                               pooling='avg',\n                               classes=100,\n                               weights='imagenet'))\n\n    elif(head_id is 'mobilenet'):\n        model.add(app.MobileNet(alpha=1.0,\n                               depth_multiplier=1,\n                               dropout=0.001,\n                               include_top=False,\n                               weights=\"imagenet\",\n                               input_tensor=None,\n                               input_shape = sshape,\n                               pooling=None,\n                               classes=1000))\n\n    elif(head_id is 'inception'):\n        model.add(InceptionV3(input_shape = sshape, \n                                 include_top = False, \n                                 weights = 'imagenet'))\n\n    elif(head_id is 'efficientnet'):\n        model.add(EfficientNetB4(input_shape = sshape, \n                                    include_top = False, \n                                    weights = 'imagenet'))\n\n    ''' Tail Model Part '''\n    model.add(Flatten())\n    model.add(Dense(1024,activation='relu'))\n    model.add(Dropout(0.01))\n    model.add(Dense(labels,activation='softmax'))\n\n    # # freeze main model coefficients\n    model.layers[0].trainable = False\n#     model.summary()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Define Model Architectre '''\nlst_heads = ['vgg','resnet','mobilenet','inception','efficientnet']\n# lst_heads = ['mobilenet']\nverbose = True\n\nlst_res_train = [] ; lst_res_val = []\nfor head_id in lst_heads:\n\n    # define CNN head model\n    model = pretrained_model(head_id)\n\n    ''' Callback Options During Training '''\n    callbacks = [ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=0, \n                                   factor=0.5,mode='max',min_lr=0.001),\n                 ModelCheckpoint(filepath=f'model_{head_id}.h5',monitor='val_accuracy',\n                                 mode = 'max',verbose=0,save_best_only=True),\n                 TqdmCallback(verbose=1)] \n\n    ''' Model Compilation '''\n    # Let's use Adam optimiser,categorical_crossentropy for the loss function & accuracy for the evaluation metric. \n    model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n\n    ''' Start Training '''\n    start = time.time()\n    history = model.fit(gen_train,\n                        validation_data = gen_valid,\n                        callbacks=callbacks,\n                        verbose=0,epochs=n_epochs)\n    end = time.time()\n    if(verbose):\n        print(f'Head Model: {head_id}')\n        print(f'The time taken to execute is {round(end-start,2)} seconds.')\n        print(f'Maximum Train/Val {max(history.history[\"accuracy\"]):.4f}/{max(history.history[\"val_accuracy\"]):.4f}')\n        \n    # store results \n    lst_res_train.append(history.history['accuracy'])  \n    lst_res_val.append(history.history['val_accuracy']) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2,subplot_titles=['Training Accuracy','Validation Accuracy'])\nii=-1\nfor i in lst_heads:\n    ii+=1;fig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)], \n                                   y=lst_res_train[ii],\n                                   name=f'train_{i}',\n                                   line=dict(color=lst_color[ii])),row=1, col=1)\nii=-1\nfor i in lst_heads:\n    ii+=1;fig.add_trace(go.Scatter(x=[i for i in range(0,n_epochs)], \n                                   y=lst_res_val[ii],\n                                   name=f'val_{i}',\n                                   line=dict(color=lst_color[ii])),row=1, col=2)\n    \nfig.update_layout(template='plotly_white',\n                  title='<b>Established Models : Evaluation Metric (Accuracy)</b>',\n                 font=dict(family='sans-serif',size=14))\nfig.update_layout(margin={\"r\":0,\"t\":100,\"l\":0,\"b\":0},height=400,showlegend=False)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SUMMARY\n- Pretrained models are built in a way that allows us to load and modify the tail end of the model to fit our own number of label case.\n- A simplistic tail end dense layer was added, however it most likely is not optimal, and neither are the predefined settings for each class, nevertheless it was interesting to compare the different models to one another and compare their performance.\n- <b>VGG</b>, <b>InceptionV3</b> and most notably <b>MobileNet</b> were among the best performing models. In this case, we can see that model performance for both training and test showed a high nature of correlation, <b>MobileNet</b> was able to reach a validation accuracy of 0.975. Due to the large nature of customisability in such problems, it's quite likely that further improvement is possible given the right combination of augmentations are applied.","metadata":{}},{"cell_type":"markdown","source":"# 4. <span style='color:#B6DA32 '>INFERENCE</span>\n## <span style='color:#73C382'> <sub>TEST IMAGE INFERENCE</sub> </span>\n- Having created a model & verified that our model has a <b>good balance between training & validation accuracy</b>, let's see how well the model performs on unseed data.","metadata":{}},{"cell_type":"code","source":"for head_id in lst_heads:\n    print(f'Head Model: {head_id}')\n    load_model = models.load_model(f\"model_{head_id}.h5\")\n    scores = load_model.evaluate(gen_test, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}